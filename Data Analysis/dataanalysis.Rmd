---
title: "Data Analysis"
author: "David Asare Kumi"
date: "4/15/2020"
output: html_document
---


## Data Analysis

### Introduction

- So far, every example in this book has started with a nice dataset that’s easy to plot. That’s great for learning (because you don’t want to struggle with data handling while you’re learning visualisation), but in real life, datasets hardly ever come in exactly the right structure. To use ggplot2 in practice,
you’ll need to learn some data wrangling skills. Indeed, in my experience, visualisation is often the easiest part of the data analysis process: once you have the right data, in the right format, aggregated in the right way, the right visualisation is often obvious.

- The goal of this part of the book is to show you how to integrate ggplot2 with other tools needed for a complete data analysis:


1. In this chapter, you’ll learn the principles of tidy data (Wickham, 2014), which help you organise your data in a way that makes it easy to visualise with ggplot2, manipulate with dplyr and model with the many modelling packages. The principles of tidy data are supported by the tidyr package,
which helps you tidy messy datasets.

2. Most visualisations require some data transformation whether it’s creating a new variable from existing variables, or performing simple aggregations so you can see the forest for the trees. Chapter 10 will show you how to do this with the dplyr package.

3. If you’re using R, you’re almost certainly using it for its fantastic modelling capabilities. While there’s an R package for almost every type of model that you can think of, the results of these models can be hard to visualise. In Chap. 11, you’ll learn about the broom package, by David Robinson,
to convert models into tidy datasets so you can easily visualise them withggplot2.

- Tidy data is the foundation for data manipulation and visualising models. In the following sections, you’ll learn the definition of tidy data, and the tools you need to make messy data tidy. The chapter concludes with two case studies that show how to apply the tools in sequence to work with real(istic)
data.

## Tidy Data

- The principle behind tidy data is simple: storing your data in a consistent way makes it easier to work with it. Tidy data is a mapping between the statistical structure of a data frame (variables and observations) and the physical structure (columns and rows). Tidy data follows two main principles:

1. Variables go in columns.

2. Observations go in rows.

- Tidy data is particularly important for ggplot2 because the job of ggplot2 is to map variables to visual properties: if your data isn’t tidy, you’ll have a hard time visualising it.

- Sometimes you’ll find a dataset that you have no idea how to plot. That’s normally because it’s not tidy. For example, take this data frame that contains monthly employment data for the United States:

-  month is stored in a column.

-  year is spread across the column names.

-  rate is the value of each cell.

- To make it possible to plot this data we first need to tidy it. There are two important pairs of tools:

1. Spread & gather.

2. Separate & unite.

## Spread and Gather

- Tidying your data will often require translating Cartesian indexed forms, called gathering, and less commonly, indexed Cartesian, called spreading. The tidyr package provides the spread() and gather() functions to perform these operations, as described below.

## Gather

- gather() has four main arguments:

1. data: the dataset to translate.

2. key & value: the key is the name of the variable that will be created from the column names, and the value is the name of the variable that will be created from the cell values.

3. ...: which variables to gather. You can specify individually, A, B, C, D, or as a range A:D. Alternatively, you can specify which columns are not to be gathered with -: -E, -F.

- To tidy the economics dataset shown above, you first need to identify the variables: year, month and rate. month is already in a column, but year and rate are in Cartesian form, and we want them in indexed form, so we need to use gather(). In this example, the key is year, the value is unemp and we
want to select columns from 2006 to 2015:

- gather(data,key,value,columns to be selected).

- gather(ec2,key=year,value=unemp,`2006`:`2015`).

- Note that the columns have names that are not standard variable names in R (they don’t start with a letter). This means that we need to surround them in backticks, i.e. ‘2006‘ to refer to them.

- Alternatively, we could gather all columns except month:

- gather(ec2, key = year, value = unemp, -month)

- To be most useful, we can provide two extra arguments:

- economics_2 <- gather(ec2, year, rate, `2006`:`2015`, convert = TRUE, na.rm = TRUE)
economics_2

- We use convert = TRUE to automatically convert the years from character strings to numbers, and na.rm = TRUE to remove the months with no data. (In some sense the data isn’t actually missing because it represents dates that haven’t occurred yet.)

- When the data is in this form, it’s easy to visualise in many different ways. For example, we can choose to emphasise either long term trend or seasonal variations:

- ggplot(economics_2, aes(year + (month - 1) / 12, rate)) + geom_line()

- ggplot(economics_2, aes(month, rate, group = year)) + geom_line(aes(colour = year), size = 1)


## Spread

- spread() is the opposite of gather(). You use it when you have a pair of columns that are in indexed form, instead of Cartesian form. For example, the following example dataset contains three variables (day, rain and temp), but rain and temp are stored in indexed form.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

weather <- dplyr::data_frame(
day = rep(1:3, 2),
obs = rep(c("temp", "rain"), each = 3),
val = c(c(23, 22, 20), c(0, 0, 5))
)
weather

```

- Spread allows us to turn this messy indexed form into a tidy Cartesian form. It shares many of the arguments with gather(). You’ll need to supply the data to translate, as well as the name of the key column which gives the variable names, and the value column which contains the cell values. Here the key is obs and the value is val:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

weather <- dplyr::data_frame(
day = rep(1:3, 2),
obs = rep(c("temp", "rain"), each = 3),
val = c(c(23, 22, 20), c(0, 0, 5))
)

spread(weather, key = obs, value = val)

```

## Separate and Unite

- Spread and gather help when the variables are in the wrong place in the dataset. Separate and unite help when multiple variables are crammed into one column, or spread across multiple columns.

- For example, the following dataset stores some information about the response to a medical treatment. There are three variables (time, treatment and value), but time and treatment are jammed in one variable together:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

trt <- dplyr::data_frame(
var = paste0(rep(c("beg", "end"), each = 3), "_", rep(c("a", "b", "c"))),
val = c(1, 4, 2, 10, 5, 11)
)
trt

```

- The separate() function makes it easy to tease apart multiple variables stored in one column. It takes four arguments:

1. data: the data frame to modify.

2. col: the name of the variable to split into pieces.

3. into: a character vector giving the names of the new variables.

4. sep: a description of how to split the variable apart. This can either be a regular expression, e.g. to split by underscores, or [ˆa-z] to split by any non-letter, or an integer giving a position.

- In this case, we want to split by the character:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

separate(trt, var, c("time", "treatment"), "_")


```

- (If the variables are combined in a more complex form, have a look at extract(). Alternatively, you might need to create columns individually yourself using other calculations. A useful tool for this is mutate() which you’ll learn about in the next chapter.)

- unite() is the inverse of separate()—it joins together multiple columns into one column. This is less common, but it’s useful to know about as the inverse of separate().

## Case Studies

- For most real datasets, you’ll need to use more than one tidying verb. There many be multiple ways to get there, but as long as each step makes the data tidier, you’ll eventually get to the tidy dataset. That said, you typically apply the functions in the same order: gather(), separate() and spread() (although you might not use all three).


## Blood Pressure

- The first step when tidying a new dataset is always to identify the variables. Take the following simulated medical data. There are seven variables in this dataset: name, age, start date, week, systolic & diastolic blood pressure. Can you see how they’re stored?


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

# Adapted from example by Barry Rowlingson,
# http://barryrowlingson.github.io/hadleyverse/

bpd <- readr::read_table(
"name age start week1 week2 week3
Anne 35 2014-03-27 100/80 100/75 120/90
Ben 41 2014-03-09 110/65 100/65 135/70
Carl 33 2014-04-02 125/80 NA NA
", na = "<NA>")

#The first step is to convert from Cartesian to indexed form:
#bpd_1 <- gather(bpd, week, bp, week1:week3)
#bpd_1

#> Source: local data frame [9 x 5]
#>
#> name age start week bp
#> (chr) (int) (date) (chr) (chr)
#> 1 Anne 35 2014-03-27 week1 100/80
#> 2 Ben 41 2014-03-09 week1 110/65
#> 3 Carl 33 2014-04-02 week1 125/80
#> 4 Anne 35 2014-03-27 week2 100/75
#> 5 Ben 41 2014-03-09 week2 100/65
#> 6 Carl 33 2014-04-02 week2 NA
#> .. ... ... ... ... ...

```

- This is tidier, but we have two variables combined together in the bp variable. This is a common way of writing down the blood pressure, but analysis is easier if we break it into two variables. That’s the job of separate:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

#bpd_2 <- separate(bpd_1, bp, c("sys", "dia"), "/")
#bpd_2
#> Source: local data frame [9 x 6]
#>
#> name age start week sys dia
#> (chr) (int) (date) (chr) (chr) (chr)
#> 1 Anne 35 2014-03-27 week1 100 80
#> 2 Ben 41 2014-03-09 week1 110 65
#> 3 Carl 33 2014-04-02 week1 125 80
#> 4 Anne 35 2014-03-27 week2 100 75
#> 5 Ben 41 2014-03-09 week2 100 65
#> 6 Carl 33 2014-04-02 week2 NA NA
#> .. ... ... ... ... ... ...

```

- This dataset is now tidy, but we could do a little more to make it easier to use. The following code uses extract() to pull the week number out into its own variable (using regular expressions is beyond the scope of the book, but \\d stands for any digit). I also use arrange() (which you’ll learn about
in the next chapter) to order the rows to keep the records for each person together.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

#bpd_3 <- extract(bpd_2, week, "week", "(\\d)", convert = TRUE)
#bpd_4 <- dplyr::arrange(bpd_3, name, week)
#bpd_4
#> Source: local data frame [9 x 6]
#>
#> name age start week sys dia
#> (chr) (int) (date) (int) (chr) (chr)
#> 1 Anne 35 2014-03-27 1 100 80
#> 2 Anne 35 2014-03-27 2 100 75
#> 3 Anne 35 2014-03-27 3 120 90
#> 4 Ben 41 2014-03-09 1 110 65
#> 5 Ben 41 2014-03-09 2 100 65
#> 6 Ben 41 2014-03-09 3 135 70
#> .. ... ... ... ... ... ...

```

- You might notice that there’s some repetition in this dataset: if you know the name, then you also know the age and start date. This reflects a third condition of tidyness that I don’t discuss here: each data frame should contain one and only one data set. Here there are really two datasets: information about each person that doesn’t change over time, and their weekly blood pressure measurements. You can learn more about this sort of messiness in the resources mentioned at the end of the chapter.

## Test Scores

- Imagine you’re interested in the effect of an intervention on test scores. You’ve collected the following data. What are the variables?


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

# Adapted from http://stackoverflow.com/questions/29775461
scores <- dplyr::data_frame(
person = rep(c("Greg", "Sally", "Sue"), each = 2),
time = rep(c("pre", "post"), 3),
test1 = round(rnorm(6, mean = 80, sd = 4), 0),
test2 = round(jitter(test1, 15), 0)
)
scores

```

- I think the variables are person, test, pre-test score and post-test score. As usual, we start by converting columns in Cartesian form (test1 and test2) to indexed form (test and score):

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

scores_1 <- gather(scores, test, score, test1:test2)
scores_1

```

- Now we need to do the opposite: pre and post should be variables, not values, so we need to spread time and score:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

scores_2 <- spread(scores_1, time, score)
scores_2

```

- A good indication that we have made a tidy dataset is that it’s now easy to calculate the statistic of interest: the difference between pre- and postintervention scores:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

scores_2 <- spread(scores_1, time, score)
scores_3 <- mutate(scores_2, diff = post - pre)
scores_3

```

- And it’s similarly easy to plot:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

ggplot(scores_3, aes(person, diff, color = test)) +
geom_hline(size = 2, colour = "white", yintercept = 0) +
geom_point() +
geom_path(aes(group = person), colour = "grey50",
arrow = arrow(length = unit(0.25, "cm")))

```

- Again, you’ll learn about mutate() in the next chapter.


## Learning More

- Data tidying is a big topic and this chapter only scratches the surface. I recommend the following references which go into considerably more depth on this topic:

## References

1. Wickham H (2007) Reshaping data with the reshape package. J Stat Soft
21(12). http://www.jstatsoft.org/v21/i12/paper

2. Wickham H (2014) Tidy data. J Stat Softw 59. http://www.jstatsoft.org/
v59/i10/

3. The tidyr vignette (http://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).

4. The data wrangling cheatsheet (http://rstudio.com/cheatsheets) by RStudio, includes the most common tidyr verbs in a form designed to jog your memory when you’re stuck.

## Data Transformation

### Introduction

- Tidy data is important, but it’s not the end of the road. Often you won’t have quite the right variables, or your data might need a little aggregation before you visualise it. This chapter will show you how to solve these problems (and more!) with the dplyr package.

- The goal of dplyr is to provide verbs (functions) that help you solve the most common 95% of data manipulation problems. dplyr is similar to ggplot2, but instead of providing a grammar of graphics, it provides a grammar of data manipulation. Like ggplot2, dplyr helps you not just by giving you
functions, but it also helps you think about data manipulation. In particular, dplyr helps by constraining you: instead of struggling to think about which of the thousands of functions that might help, you can just pick from a handful that are design to be very likely to be helpful. In this chapter you’ll learn four of the most important dplyr verbs:

1. filter()

2. mutate()

3. group by() & summarise()

- These verbs are easy to learn because they all work the same way: they take a data frame as the first argument, and return a modified data frame. The other arguments control the details of the transformation, and are always interpreted in the context of the data frame so you can refer to variables directly. I’ll also explain each in the same way: I’ll show you a motivating example using the diamonds data, give you more details about how the function works, and finish up with some exercises for you to practice your skills with.

- You’ll also learn how to create data transformation pipelines using %>%. %>% plays a similar role to + in ggplot2: it allows you to solve complex problems by combining small pieces that are easily understood in isolation.

- This chapter only scratches the surface of dplyr’s capabilities but it should be enough to help you with visualisation problems. You can learn more by using the resources discussed at the end of the chapter.

## Filter Observations

- It’s common to only want to explore one part of a dataset. A great data analysis strategy is to start with just one observation unit (one person, one city, etc), and understand how it works before attempting to generalise the conclusion to others. This is a great technique if you ever feel overwhelmed by an analysis: zoom down to a small subset, master it, and then zoom back out, to apply your conclusions to the full dataset.

- Filtering is also useful for extracting outliers. Generally, you don’t want to just throw outliers away, as they’re often highly revealing, but it’s useful to think about partitioning the data into the common and the unusual. You summarise the common to look at the broad trends; you examine the outliers
individually to see if you can figure out what’s going on.

- For example, look at this plot that shows how the x and y dimensions of the diamonds are related:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

ggplot(diamonds, aes(x, y)) + geom_bin2d()

```

- There are around 50,000 points in this dataset: most of them lie along the diagonal, but there are a handful of outliers. One clear set of incorrect values are those diamonds with zero dimensions. We can use filter() to pull them out:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

filter(diamonds, x == 0 | y == 0)

```

- This is equivalent to the base R code diamonds[diamonds$x == 0 | diamonds$y == 0, ], but is more concise because filter() knows to look for the bare x in the data frame.

- (If you’ve used subset() before, you’ll notice that it has very similar behaviour. The biggest difference is that subset() can select both observations and variables, where in dplyr, filter() works exclusively with observations and select() with variables. There are some other subtle differences, but the main advantage to using filter() is that it behaves identically to the other dplyr verbs and it tends to be a bit faster than subset().)

- In a real analysis, you’d look at the outliers in more detail to see if you can find the root cause of the data quality problem. In this case, we’re just going to throw them out and focus on what remains. To save some typing, we may provide multiple arguments to filter() which combines them.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

diamonds_ok <- filter(diamonds, x > 0, y > 0, y < 20)
ggplot(diamonds_ok, aes(x, y)) +
geom_bin2d() +
geom_abline(slope = 1, colour = "white", size = 1, alpha = 0.5)

```

- This plot is now more informative—we can see a very strong relationship between x and y. I’ve added the reference line to make it clear that for most diamonds, x and y are very similar. However, this plot still has problems:

1. The plot is mostly empty, because most of the data lies along the diagonal.

2. There are some clear bivariate outliers, but it’s hard to select them with a simple filter.

- We’ll solve both of these problem in the next section by adding a new variable that’s a transformation of x and y. But before we continue on to that, let’s talk more about the details of filter().

## Useful Tools

- The first argument to filter() is a data frame. The second and subsequent arguments must be logical vectors: filter() selects every row where all the logical expressions are TRUE. The logical vectors must always be the same length as the data frame: if not, you’ll get an error. Typically you create the logical vector with the comparison operators:

1. x == y: x and y are equal.

2. x != y: x and y are not equal.

3. x %in% c("a", "b", "c"): x is one of the values in the right hand side.

4. x > y, x >= y, x < y, x <= y: greater than, greater than or equal to, less than, less than or equal to.

- And combine them with logical operators:

1. !x (pronounced “not x”), flips TRUE and FALSE so it keeps all the values where x is FALSE.

2. x & y: TRUE if both x and y are TRUE.

3. x | y: TRUE if either x or y (or both) are TRUE.

4. xor(x, y): TRUE if either x or y are TRUE, but not both (exclusive or).

- Most real queries involve some combination of both:

1. Price less than $500: price < 500

2. Size between 1 and 2 carats: carat >= 1 & carat < 2

3. Cut is ideal or premium: cut == "Premium" | cut == "Ideal", or cut %in% c("Premium", "Ideal") (note that R is case sensitive)

4. Worst colour, cut and clarity: cut == "Fair" & color == "J" & clarity =="SI2"

- You can also use functions in the filtering expression:

1. Size is between 1 and 2 carats: floor(carat) == 1

2. An average dimension greater than 3: (x + y + z) / 3 > 3

- This is useful for simple expressions, but as things get more complicated it’s better to create a new variable first so you can check that you’ve done the computation correctly before doing the subsetting. You’ll learn how to do that in the next section.

- The rules for NA are a bit trickier, so I’ll explain them next.

## Missing Values

- NA, R’s missing value indicator, can be frustrating to work with. R’s underlying philosophy is to force you to recognise that you have missing values, and make a deliberate choice to deal with them: missing values never silently go missing. This is a pain because you almost always want to just get rid of them, but it’s a good principle to force you to think about the correct option.

- The most important thing to understand about missing values is that they are infectious: with few exceptions, the result of any operation that includes a missing value will be a missing value. This happens because NA represents an unknown value, and there are few operations that turn an unknown value into a known value.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

x <- c(1, NA, 2)
x == 1

x > 2

x + 10

```

- When you first learn R, you might be tempted to find missing values using==:

- But that doesn’t work! A little thought reveals why: there’s no reason why two unknown values should be the same. Instead, use is.na(X) to determine if a value is missing:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

x <- c(1, NA, 2)

is.na(x)

```

- filter() only includes observations where all arguments are TRUE, so NA values are automatically dropped. If you want to include missing values, be explicit: x > 10 | is.na(x). In other parts of R, you’ll sometimes need to convert missing values into FALSE. You can do that with x > 10 & !is.na(x).

## Create New Variables

- To better explore the relationship between x and y, it’s useful to “rotate” the plot so that the data is flat, not diagonal.We can do that by creating two new variables: one that represents the difference between x and y (which in this context represents the symmetry of the diamond) and one that represents its size (the length of the diagonal).

- To create new variables use mutate(). Like filter() it takes a data frame as its first argument and returns a data frame. Its second and subsequent arguments are named expressions that generate new variables. Like filter() you can refer to variables just by their name, you don’t need to also include
the name of the dataset.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
data(economics)

diamonds_ok2 <- mutate(diamonds_ok,
sam = x - y,
size = sqrt(x ^ 2 + y ^ 2)
)
diamonds_ok2

ggplot(diamonds_ok2, aes(size, sam)) + stat_bin2d()

```

- This plot has two advantages: we can more easily see the pattern followed by most diamonds, and we can easily select outliers. Here, it doesn’t seem important whether the outliers are positive (i.e. x is bigger than y) or negative (i.e. y is bigger x). So we can use the absolute value of the symmetry variable to pull out the outliers. Based on the plot, and a little experimentation, I came up with a threshold of 0.20. We’ll check out the results with a histogram.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

g1<-ggplot(diamonds_ok2, aes(abs(sam))) +
geom_histogram(binwidth = 0.10)
diamonds_ok3 <- filter(diamonds_ok2, abs(sam) < 0.20)

g2<-ggplot(diamonds_ok3, aes(abs(sam))) +
geom_histogram(binwidth = 0.01)

grid.arrange(g1,g2,ncol=2)

```

- That’s an interesting histogram! While most diamonds are close to being symmetric there are very few that are perfectly symmetric (i.e. x == y).

## Useful Tools

- Typically, transformations will be suggested by your domain knowledge. However, there are a few transformations that are useful in a surprisingly wide range of circumstances.

1. Log-transformations are often useful. They turn multiplicative relationships into additive relationships; they compress data that varies over orders of magnitude; they convert power relationships to linear relationship. See examples at http://stats.stackexchange.com/questions/27951

2. Relative difference: If you’re interested in the relative difference between two variables, use log(x / y). It’s better than x / y because it’s symmetric: if x < y, x / y takes values [0, 1), but if x > y, x / y takes values (1, Inf). See Tornqvist et al. (1985) for more details.

3. Sometimes integrating or differentiating might make the data more interpretable: if you have distance and time, would speed or acceleration be more useful? (or vice versa). (Note that integration makes data more smooth; differentiation makes it less smooth.)

4. Partition a number into magnitude and direction with abs(x) and sign(x).
There are also a few useful ways to transform pairs of variables:

5. Partitioning into overall size and difference is often useful, as seen above.

6. If you see a strong trend, use a model to partition it into pattern and residuals is often useful. You’ll learn more about that in the next chapter.

7. Sometimes it’s useful to change positions to polar coordinates (or vice versa): distance (sqrt(xˆ2 + yˆ2)) and angle (atan2(y, x)).

## Group-wise Summaries

- Many insightful visualisations require that you reduce the full dataset down to a meaningful summary. ggplot2 provides a number of geoms that will do summaries for you. But it’s often useful to do summaries by hand: that gives you more flexibility and you can use the summaries for other purposes. dplyr does summaries in two steps:

1. Define the grouping variables with group by().

2. Describe how to summarise each group with a single row with summarise().

- For example, to look at the average price per clarity, we first group by clarity, then summarise:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

by_clarity <- group_by(diamonds, clarity)
sum_clarity <- summarise(by_clarity, price = mean(price))
sum_clarity

ggplot(sum_clarity, aes(clarity, price)) +
geom_line(aes(group = 1), colour = "grey80") +
geom_point(size = 2)

```

- You might be surprised by this pattern: why do diamonds with better clarity have lower prices? We’ll see why this is the case and what to do about it in Sect. 11.2.

- Supply additional variables to group by() to create groups based on more than one variable. The next example shows how we can compute (by hand) a frequency polygon that shows how cut and depth interact. The special summary function n() counts the number of observations in each group.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

cut_depth <- summarise(group_by(diamonds, cut, depth), n = n())
cut_depth <- filter(cut_depth, depth > 55, depth < 70)
cut_depth

ggplot(cut_depth, aes(depth, n, colour = cut)) +
geom_line()

```

- We can use a grouped mutate() to convert counts to proportions, so it’s easier to compare across the cuts. summarise() strips one level of grouping off, so cut depth will be grouped by cut.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

cut_depth <- mutate(cut_depth, prop = n / sum(n))
ggplot(cut_depth, aes(depth, prop, colour = cut)) +
geom_line()

```

## Useful Tools

- summarise() needs to be used with functions that take a vector of n values and always return a single value. Those functions include:

1. Counts: n(), n distinct(x).

2. Middle: mean(x), median(x).

3. Spread: sd(x), mad(x), IQR(x).

4. Extremes: quartile(x), min(x), max(x).

5. Positions: first(x), last(x), nth(x, 2).

- Another extremely useful technique is to use sum() or mean() with a logical vector.When a logical vector is treated as numeric, TRUE becomes 1 and FALSE becomes 0. This means that sum() tells you the number of TRUEs, and mean() tells you the proportion of TRUEs. For example, the following code counts the number of diamonds with carat greater than or equal to 4, and the proportion of diamonds that cost less than $1000.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

summarise(diamonds,
n_big = sum(carat >= 4),
prop_cheap = mean(price < 1000)
)

```

- Most summary functions have a na.rm argument: na.rm = TRUE tells the summary function to remove any missing values prior to summarisation. This is a convenient shortcut: rather than removing the missing values then summarising, you can do it in one step.

## Statistical Considerations 

- When summarising with the mean or median, it’s always a good idea to include a count and a measure of spread. This helps you calibrate your assessments—if you don’t include them you’re likely to think that the data is less variable than it really is, and potentially draw unwarranted conclusions.
The following example extends our previous summary of the average price by clarity to also include the number of observations in each group, and the upper and lower quartiles. It suggests the mean might be a bad summary for this data - the distributions of price are so highly skewed that the mean is
higher than the upper quartile for some of the groups!

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
data(economics)

by_clarity <- diamonds %>%
group_by(clarity) %>%
summarise(
n = n(),
mean = mean(price),
lq = quantile(price, 0.25),
uq = quantile(price, 0.75)
)
by_clarity

ggplot(by_clarity, aes(clarity, mean)) +
geom_linerange(aes(ymin = lq, ymax = uq)) +
geom_line(aes(group = 1), colour = "grey50") +
geom_point(aes(size = n))

```

- Another example of this comes from baseball. Let’s take the MLB batting data from the Lahman package and calculate the batting average: the number of hits divided by the number of at bats. Who’s the best batter according to this metric?


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

batters <- filter(Batting, AB > 0)
per_player <- group_by(batters, playerID)
ba <- summarise(per_player,
ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE)
)
ggplot(ba, aes(ba)) +
geom_histogram(binwidth = 0.01)

```

- Wow, there are a lot of players who can hit the ball every single time! Would you want them on your fantasy baseball team? Let’s double check they’re really that good by calibrating also showing the total number of at bats:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

ba <- summarise(per_player,
ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),
ab = sum(AB, na.rm = TRUE)
)

ggplot(ba, aes(ab, ba)) +
geom_bin2d(bins = 100) +
geom_smooth()

```

- The highest batting averages occur for the players with the smallest number of at bats - it’s not hard to hit the ball every time if you’ve only had two pitches. We can make the pattern a little more clear by getting rid of the players with less than 10 at bats.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

ggplot(filter(ba, ab >= 10), aes(ab, ba)) +
geom_bin2d() +
geom_smooth()

```

- You’ll often see a similar pattern whenever you plot number of observations vs. an average. Be aware!

## Transformation Pipelines

- Most real analyses require you to string together multiple mutate()s, filter()s, group by()s , and summarise()s. For example, above, we created a frequency polygon by hand with a combination of all four verbs:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

# By using intermediate values
cut_depth <- group_by(diamonds, cut, depth)
cut_depth <- summarise(cut_depth, n = n())
cut_depth <- filter(cut_depth, depth > 55, depth < 70)
cut_depth <- mutate(cut_depth, prop = n / sum(n))

```

- This sequence of operations is a bit painful because we repeated the name of the data frame many times. An alternative is just to do it with one sequence of function calls:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

# By "composing" functions
mutate(
filter(
summarise(
group_by(
diamonds,
cut,
depth
),
n = n()
),
depth > 55,
depth < 70
),
prop = n / sum(n)
)

```

- But this is also hard to read because the sequence of operations is inside out, and the arguments to each function can be quite far apart. dplyr provides an alternative approach with the pipe, %>%. With the pipe, we can write the above sequence of operations as:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

cut_depth <- diamonds %>%
group_by(cut, depth) %>%
summarise(n = n()) %>%
filter(depth > 55, depth < 70) %>%
mutate(prop = n / sum(n))

```

- This makes it easier to understand what’s going on as we can read it almost like an English sentence: first group, then summarise, then filter, then mutate. In fact, the best way to pronounce %>% when reading a sequence of code is as “then”. %>% comes from the magrittr package, by Stefan Milton Bache. It provides a number of other tools that dplyr doesn’t expose by default, so I highly recommend that you check out the magrittr website (https://github.com/smbache/magrittr).

- %>% works by taking the thing on the left hand side (LHS) and supplying it as the first argument to the function on the right hand side (RHS). Each of these pairs of calls is equivalent:

- f(x, y) is the same as x %>% f(y)

- g(f(x, y), z) is the same as x %>% f(y) %>% g(z)


## Learning More

- dplyr provides a number of other verbs that are less useful for visualisation, but important to know about in general:

1. arrange() orders observations according to variable(s). This is most useful when you’re looking at the data from the console. It can also be useful for visualisations if you want to control which points are plotted on top.

2. select() picks variables based on their names. Useful when you have many variables and want to focus on just a few for analysis.

3. rename() allows you to change the name of variables.

4. Grouped mutates and filters are also useful, but more advanced. See vignette("window-functions", package = "dplyr") for more details.

5. There are a number of verbs designed to work with two tables of data at a time. These include SQL joins (like the base merge() function) and set operations. Learn more about them in vignette("two-table", package = "dplyr").

6. dplyr can work directly with data stored in a database - you use the same R code as you do for local data and dplyr generates SQL to send to the database. See vignette("databases", package = "dplyr") for the details.

- Finally, RStudio provides a handy dplyr cheatsheet that will help jog your memory when you’re wondering which function to use. Get it from http://rstudio.com/cheatsheets.

## Reference

- T¨ornqvist L, Pentti V, Yrj¨o OV (1985) How should relative changes be measured?
Am Stat 39(1):43–46.

## Modelling for Visualisation

### Introduction

- Modelling is an essential tool for visualisation. There are two particularly strong connections between modelling and visualisation that I want to explore in this chapter:

1. Using models as a tool to remove obvious patterns in your plots. This is useful because strong patterns mask subtler effects. Often the strongest effects are already known and expected, and removing them allows you to see surprises more easily.

2. Other times you have a lot of data, too much to show on a handful of plots. Models can be a powerful tool for summarising data so that you get a higher level view.

- In this chapter, I’m going to focus on the use of linear models to achieve these goals. Linear models are a basic, but powerful, tool of statistics, and I recommend that everyone serious about visualisation learns at least the basics of how to use them. To this end, I highly recommend two books by Julian J. Faraway:

1. Linear Models with R http://amzn.com/1439887330

2. Extending the Linear Model with R http://amzn.com/158488424X

- These books cover some of the theory of linear models, but are pragmatic and focussed on how to actually use linear models (and their extensions) in R.

- There are many other modelling tools, which I don’t have the space to show. If you understand how linear models can help improve your visualisations, you should be able to translate the basic idea to other families of models. This chapter just scratches the surface of what you can do. But hopefully
it reinforces how visualisation can combine with modelling to help you build a powerful data analysis toolbox. For more ideas, check out Wickham et al. (2015).

- This chapter only scratches the surface of the intersection between visualisation and modelling. In my opinion, mastering the combination of visualisations and models is key to being an effective data scientist. Unfortunately most books (like this one!) only focus on either visualisation or modelling,
but not both. There’s a lot of interesting work to be done.

## Removing Trend

- So far our analysis of the diamonds data has been plagued by the powerful relationship between size and price. It makes it very difficult to see the impact of cut, colour and clarity because higher quality diamonds tend to be smaller, and hence cheaper. This challenge is often called confounding. We can use a linear model to remove the effect of size on price. Instead of looking at the raw price, we can look at the relative price: how valuable is this diamond relative to the average diamond of the same size.

- To get started, we’ll focus on diamonds of size two carats or less (96% of the dataset). This avoids some incidental problems that you can explore in the exercises if you’re interested. We’ll also create two new variables: log price and log carat. These variables are useful because they produce a plot
with a strong linear trend.


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

diamonds2 <- diamonds %>%
filter(carat <= 2) %>%
mutate(
lcarat = log2(carat),
lprice = log2(price)
)
head(diamonds2)

ggplot(diamonds2, aes(lcarat, lprice)) +
geom_bin2d() +
geom_smooth(method = "lm", se = FALSE, size = 2, colour = "yellow")

```

- In the graphic we used geom smooth() to overlay the line of best fit to the data. We can replicate this outside of ggplot2 by fitting a linear model with lm(). This allows us to find out the slope and intercept of the line:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

mod <- lm(lprice ~ lcarat, data = diamonds2)
coef(summary(mod))

```

- If you’re familiar with linear models, you might want to interpret those coefficients: log2(price) = 12.2 + 1.7 · log2(carat), which implies price = 4900 · carat1.7. Interpreting those coefficients certainly is useful, but even if you don’t understand them, the model can still be useful. We can use it to subtract the trend away by looking at the residuals: the price of each diamond minus its predicted price, based on weight alone. Geometrically, the residuals are the vertical distance between each point and the line of best fit. They tell us the price relative to the “average” diamond of that size.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

diamonds2 <- diamonds2 %>% mutate(rel_price = resid(mod))
ggplot(diamonds2, aes(carat, rel_price)) +
geom_bin2d()

```

- A relative price of zero means that the diamond was at the average price; positive means that it’s more expensive than expected (based on its size), and negative means that it’s cheaper than expected.

- Interpreting the values precisely is a little tricky here because we’ve logtransformed price. The residuals give the absolute difference (x − expected), but here we have log2(price) − log2(expectedprice), or equivalently log2 (price/expectedprice). If we “back-transform” to the original scale by applying the opposite transformation (2x) we get price/expectedprice. This makes
the values more interpretable, at the cost of the nice symmetry property of the logged values (i.e. both relatively cheaper and relatively more expensive diamonds have the same range). We can make a little table to help interpret the values:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

xgrid <- seq(-2, 1, by = 1/3)
data.frame(logx = xgrid, x = round(2 ^ xgrid, 2))

```

- This table illustrates why we used log2() rather than log(): a change of 1 unit on the logged scale, corresponding to a doubling on the original scale. For example, a rel price of −1 means that it’s half of the expected price; a relative price of 1 means that it’s twice the expected price.

- Let’s use both price and relative price to see how colour and cut affect the value of a diamond. We’ll compute the average price and average relative price for each combination of colour and cut:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

color_cut <- diamonds2 %>%
group_by(color, cut) %>%
summarise(
price = mean(price),
rel_price = mean(rel_price)
)
color_cut

```

- If we look at price, it’s hard to see how the quality of the diamond affects the price. The lowest quality diamonds (fair cut with colour J) have the highest average value! This is because those diamonds also tend to be larger: size and quality are confounded.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

ggplot(color_cut, aes(color, price)) +
geom_line(aes(group = cut), colour = "grey80") +
geom_point(aes(colour = cut))

```

- If however, we plot the relative price, you see the pattern that you expect: as the quality of the diamonds decreases, the relative price decreases. The worst quality diamond is 0.61x (2−0.7) the price of an “average” diamond.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(Batting)

ggplot(color_cut, aes(color, rel_price)) +
geom_line(aes(group = cut), colour = "grey80") +
geom_point(aes(colour = cut))

```

- This technique can be employed in a wide range of situations. Wherever you can explicitly model a strong pattern that you see in a plot, it’s worthwhile to use a model to remove that strong pattern so that you can see what interesting trends remain.



## Texas Housing Data

- We’ll continue to explore the connection between modelling and visualisation with the txhousing dataset:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)
head(txhousing)


ggplot(txhousing, aes(date, sales)) +
geom_line(aes(group = city), alpha = 1/2)

```

- Two factors make it hard to see the long-term trend in this plot:

1. The range of sales varies over multiple orders of magnitude. The biggest city, Houston, averages over ~4000 sales per month; the smallest city, San Marcos, only averages ~20 sales per month.

2. There is a strong seasonal trend: sales are much higher in the summer than in the winter.

- We can fix the first problem by plotting log sales:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)

ggplot(txhousing, aes(date, log(sales))) +
geom_line(aes(group = city), alpha = 1/2)

```

- We can fix the second problem using the same technique we used for removing the trend in the diamonds data: we’ll fit a linear model and look at the residuals. This time we’ll use a categorical predictor to remove the month effect. First we check that the technique works by applying it to a single city. It’s always a good idea to start simple so that if something goes wrong you
can more easily pinpoint the problem.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)

abilene <- txhousing %>% filter(city == "Abilene")

g1<-ggplot(abilene, aes(date, log(sales))) +
geom_line()

mod <- lm(log(sales) ~ factor(month), data = abilene)
abilene$rel_sales <- resid(mod)

g2<-ggplot(abilene, aes(date, rel_sales)) +
geom_line()

grid.arrange(g1,g2,ncol=2)

```

- We can apply this transformation to every city with group by() and mutate(). Note the use of na.action = na.exclude argument to lm(). Counterintuitively this ensures that missing values in the input are matched with missing values in the output predictions and residuals. Without this argument,
missing values are just dropped, and the residuals don’t line up with the inputs.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)

deseas <- function(x, month) {
resid(lm(x ~ factor(month), na.action = na.exclude))
}
txhousing <- txhousing %>%
group_by(city) %>%
mutate(rel_sales = deseas(log(sales), month))

```

- With this data in hand, we can re-plot the data. Now that we have logtransformed the data and removed the strong seasonal effects we can see there is a strong common pattern: a consistent increase from 2000–2007, a drop until 2010 (with quite some noise), and then a gradual rebound. To make that more clear, I included a summary line that shows the mean relative sales across all cities.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)

deseas <- function(x, month) {
resid(lm(x ~ factor(month), na.action = na.exclude))
}
txhousing <- txhousing %>%
group_by(city) %>%
mutate(rel_sales = deseas(log(sales), month))

ggplot(txhousing, aes(date, rel_sales)) +
geom_line(aes(group = city), alpha = 1/5) +
geom_line(stat = "summary", fun.y = "mean", colour = "red")

```

- (Note that removing the seasonal effect also removed the intercept - we see the trend for each city relative to its average number of sales.)


## Visualising Models

- The previous examples used the linear model just as a tool for removing trend: we fit the model and immediately threw it away. We didn’t care about the model itself, just what it could do for us. But the models themselves contain useful information and if we keep them around, there are many new
problems that we can solve:

1. We might be interested in cities where the model didn’t fit well: a poorly fitting model suggests that there isn’t much of a seasonal pattern, which contradicts our implicit hypothesis that all cities share a similar pattern.

2. The coefficients themselves might be interesting. In this case, looking at the coefficients will show us how the seasonal pattern varies between cities.

3. We may want to dive into the details of the model itself, and see exactly what it says about each observation. For this data, it might help us find suspicious data points that might reflect data entry errors.

- To take advantage of this data, we need to store the models. We can do this using a new dplyr verb: do(). It allows us to store the result of arbitrary computation in a column. Here we’ll use it to store that linear model:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
data(txhousing)

models <- txhousing %>%
group_by(city) %>%
do(mod = lm(
log2(sales) ~ factor(month),
data = .,
na.action = na.exclude
))
models

```

- There are two important things to note in this code:

1. do() creates a new column called mod. This is a special type of column: instead of containing an atomic vector (a logical, integer, numeric, or character) like usual, it’s a list. Lists are R’s most flexible data structure and can hold anything, including linear models.

2. . is a special pronoun used by do(). It refers to the “current” data frame. In this example, do() fits the model 46 times (once for each city), each time replacing . with the data for one city.

- To visualise these models, we’ll turn them into tidy data frames. We’ll do that with the broom package by David Robinson.

- library(broom)

- Broom provides three key verbs, each corresponding to one of the challenges outlined above:

1. glance() extracts model-level summaries with one row of data for each model. It contains summary statistics like the R2 and degrees of freedom.

2. tidy() extracts coefficient-level summaries with one row of data for each coefficient in each model. It contains information about individual coefficients like their estimate and standard error.

3. augment() extracts observation-level summaries with one row of data for each observation in each model. It includes variables like the residual and influence metrics useful for diagnosing outliers.

- We’ll learn more about each of these functions in the following three sections.

## Model-Level Summaries

- We’ll begin by looking at how well the model fit to each city with glance():


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)


model_sum <- models %>% glance(mod)
model_sum


```

- This creates a variable with one row for each city, and variables that either summarise complexity (e.g. df) or fit (e.g. r.squared, p.value, AIC). Since all the models we fit have the same complexity (12 terms: one for each month), we’ll focus on the model fit summaries. R2 is a reasonable place to start because it’s well known. We can use a dot plot to see the variation across cities:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

ggplot(model_sum, aes(r.squared, reorder(city, r.squared))) +
geom_point()

```

- It’s hard to picture exactly what those values of R2 mean, so it’s helpful to pick out a few examples. The following code extracts and plots out the three cities with the highest and lowest R2:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

top3 <- c("Bryan-College Station", "Lubbock", "NE Tarrant County")
bottom3 <- c("McAllen", "Brownsville", "Harlingen")
extreme <- txhousing %>% ungroup() %>%
filter(city %in% c(top3, bottom3), !is.na(sales)) %>%
mutate(city = factor(city, c(top3, bottom3)))

ggplot(extreme, aes(month, log(sales))) +
geom_line(aes(group = year)) +
facet_wrap(~city)

```

- The cities with low R2 have weaker seasonal patterns and more variation between years. The data for Harlingen seems particularly noisy.


## Coefficient-Level Summaries

- The model fit summaries suggest that there are some important differences in seasonality between the different cities. Let’s dive into those differences by using tidy() to extract detail about each individual coefficient:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

coefs <- models %>% tidy(mod)
coefs

```

- We’re more interested in the month effect, so we’ll do a little extra tidying to only look at the month coefficients, and then to extract the month value into a numeric variable:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

months <- coefs %>%
filter(grepl("factor", term)) %>%
tidyr::extract(term, "month", "(\\d+)", convert = TRUE)
months

```

- This is a common pattern. You need to use your data tidying skills at many points in an analysis. Once you have the correct tidy dataset, creating the plot is usually easy. Here we’ll put month on the x-axis, estimate on the y-axis, and draw one line for each city. I’ve back-transformed to make the coefficients more interpretable: these are now ratios of sales compared to January.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

ggplot(months, aes(month, 2 ^ estimate)) +
geom_line(aes(group = city))

```

- The pattern seems similar across the cities. The main difference is the strength of the seasonal effect. Let’s pull that out and plot it:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

coef_sum <- months %>%
group_by(city) %>%
summarise(max = max(estimate))
ggplot(coef_sum, aes(2 ^ max, reorder(city, max))) +
geom_point()

```

- The cities with the strongest seasonal effect are College Station and San Marcos (both college towns) and Galveston and South Padre Island (beach cities). It makes sense that these cities would have very strong seasonal effects.

## Observation Data

- Observation-level data, which include residual diagnostics, is most useful in the traditional model fitting scenario, because it can helps you find “highleverage” points, point that have a big influence on the final model. It’s also useful in conjunction with visualisation, particularly because it provides an alternative way to access the residuals.

- Extracting observation-level data is the job of the augment() function. This adds one row for each observation. It includes the variables used in the original model, the residuals, and a number of common influence statistics (see?augment.lm for more details):

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

obs_sum <- models %>% augment(mod)
obs_sum

```

- For example, it might be interesting to look at the distribution of standardised residuals. (These are residuals standardised to have a variance of one in each model, making them more comparable.) We’re looking for unusual values that might need deeper exploration:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

g1<-ggplot(obs_sum, aes(.std.resid)) +
geom_histogram(binwidth = 0.1)

g2<-ggplot(obs_sum, aes(abs(.std.resid))) +
geom_histogram(binwidth = 0.1)

grid.arrange(g1,g2,ncol=2)

```

- A threshold of 2 seems like a reasonable threshold to explore individually:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

obs_sum %>%
filter(abs(.std.resid) > 2) %>%
group_by(city) %>%
summarise(n = n(), avg = mean(abs(.std.resid))) %>%
arrange(desc(n))

```

- In a real analysis, you’d want to look into these cities in more detail.

## Reference

- Wickham H, Dianne C, Heike H (2015) Visualizing statistical models: removing the blindfold. Stat Anal Data Min: ASA Data Sci J 8(4):203–25.


## Programming with ggplot2

### Introduction

- A major requirement of a good data analysis is flexibility. If your data changes, or you discover something that makes you rethink your basic assumptions, you need to be able to easily change many plots at once. The main inhibitor of flexibility is code duplication. If you have the same plotting
statement repeated over and over again, you’ll have to make the same change in many different places. Often just the thought of making all those changes is exhausting! This chapter will help you overcome that problem by showing you how to program with ggplot2.

- To make your code more flexible, you need to reduce duplicated code by writing functions. When you notice you’re doing the same thing over and over again, think about how you might generalise it and turn it into a function. If you’re not that familiar with how functions work in R, you might want to
brush up your knowledge at http://adv-r.had.co.nz/Functions.html.

- In this chapter I’ll show how to write functions that create:

1. A single ggplot2 component.

2. Multiple ggplot2 components.

3. A complete plot.

- And then I’ll finish off with a brief illustration of how you can apply functional programming techniques to ggplot2 objects.

- You might also find the cowplot (https://github.com/wilkelab/cowplot) and ggthemes (https://github.com/jrnold/ggthemes) packages helpful.

- As well as providing reusable components that help you directly, you can also read the source code of the packages to figure out how they work.

## Single Components

- Each component of a ggplot plot is an object. Most of the time you create the component and immediately add it to a plot, but you don’t have to. Instead, you can save any component to a variable (giving it a name), and then add it to multiple plots:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

bestfit <- geom_smooth(
method = "lm",
se = FALSE,
colour = alpha("steelblue", 0.5),
size = 2
)

g1<-ggplot(mpg, aes(cty, hwy)) +
geom_point() +
bestfit

g2<-ggplot(mpg, aes(displ, hwy)) +
geom_point() +
bestfit

grid.arrange(g1,g2,ncol=2)

```

- That’s a great way to reduce simple types of duplication (it’s much better than copying-and-pasting!), but requires that the component be exactly the same each time. If you need more flexibility, you can wrap these reusable snippets in a function. For example, we could extend our bestfit object to a more general function for adding lines of best fit to a plot. The following
code creates a geom lm() with three parameters: the model formula, the line colour and the line size:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

geom_lm <- function(formula = y ~ x, colour = alpha("steelblue", 0.5),
size = 2, ...) {
geom_smooth(formula = formula, se = FALSE, method = "lm", colour = colour,
size = size, ...)
}

g1<-ggplot(mpg, aes(displ, 1 / hwy)) +
geom_point() +
geom_lm()

g2<-ggplot(mpg, aes(displ, 1 / hwy)) +
geom_point() +
geom_lm(y ~ poly(x, 2), size = 1, colour = "red")

grid.arrange(g1,g2,ncol=2)

```

- Pay close attention to the use of “...”. When included in the function definition “...” allows a function to accept arbitrary additional arguments. Inside the function, you can then use “...” to pass those arguments on to another function. Here we pass “...” onto geom smooth() so the user can still modify all the other arguments we haven’t explicitly overridden. When you write your own component functions, it’s a good idea to always use “...” in this way.

- Finally, note that you can only add components to a plot; you can’t modify or remove existing objects.

## Multiple Components

- It’s not always possible to achieve your goals with a single component. Fortunately, ggplot2 has a convenient way of adding multiple components to a plot in one step with a list. The following function adds two layers: one to show the mean, and one to show its standard error:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

geom_mean <- function() {
list(
stat_summary(fun.y = "mean", geom = "bar", fill = "grey70"),
stat_summary(fun.data = "mean_cl_normal", geom = "errorbar", width = 0.4)
)
}

g1<-ggplot(mpg, aes(class, cty)) + geom_mean()
g2<-ggplot(mpg, aes(drv, cty)) + geom_mean()

grid.arrange(g1,g2,ncol=2)

```

- If the list contains any NULL elements, they’re ignored. This makes it easy to conditionally add components:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

geom_mean <- function(se = TRUE) {
list(
stat_summary(fun.y = "mean", geom = "bar", fill = "grey70"),
if (se)
stat_summary(fun.data = "mean_cl_normal", geom = "errorbar", width = 0.4)
)
}

g1<-ggplot(mpg, aes(drv, cty)) + geom_mean()
g2<-ggplot(mpg, aes(drv, cty)) + geom_mean(se = FALSE)

grid.arrange(g1,g2,ncol=2)

```

## Plot Components

- You’re not just limited to adding layers in this way. You can also include any of the following object types in the list:

1. A data.frame, which will override the default dataset associated with the plot. (If you add a data frame by itself, you’ll need to use %+%, but this is not necessary if the data frame is in a list.)

2. An aes() object, which will be combined with the existing default aesthetic mapping.

3. Scales, which override existing scales, with a warning if they’ve already been set by the user.

4. Coordinate systems and facetting specification, which override the existing settings.

5. Theme components, which override the specified components.


## Annotation

- It’s often useful to add standard annotations to a plot. In this case, your function will also set the data in the layer function, rather than inheriting it from the plot. There are two other options that you should set when you do this. These ensure that the layer is self-contained:

1. inherit.aes = FALSE prevents the layer from inheriting aesthetics from the parent plot. This ensures your annotation works regardless of what else is on the plot.

2. show.legend = FALSE ensures that your annotation won’t appear in the legend.

- One example of this technique is the borders() function built into ggplot2. It’s designed to add map borders from one of the datasets in the maps package.



```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

borders <- function(database = "world", regions = ".", fill = NA,
colour = "grey50", ...) {
df <- map_data(database, regions)
geom_polygon(
aes_(~lat, ~long, group = ~group),
data = df, fill = fill, colour = colour, ...,
inherit.aes = FALSE, show.legend = FALSE
)
}

```

## Additional Arguments

- If you want to pass additional arguments to the components in your function, ... is no good: there’s no way to direct different arguments to different components. Instead, you’ll need to think about how you want your function to work, balancing the benefits of having one function that does it all vs. the cost of having a complex function that’s harder to understand.

- To get you started, here’s one approach using modifyList() and do.call():

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

geom_mean <- function(..., bar.params = list(), errorbar.params = list()) {
params <- list(...)
bar.params <- modifyList(params, bar.params)
errorbar.params <- modifyList(params, errorbar.params)
bar <- do.call("stat_summary", modifyList(
list(fun.y = "mean", geom = "bar", fill = "grey70"),
bar.params)
)
errorbar <- do.call("stat_summary", modifyList(
list(fun.data = "mean_cl_normal", geom = "errorbar", width = 0.4),
errorbar.params)
)
list(bar, errorbar)
}
g1<-ggplot(mpg, aes(class, cty)) +
geom_mean(
colour = "steelblue",
errorbar.params = list(width = 0.5, size = 1)
)
g2<-ggplot(mpg, aes(class, cty)) +
geom_mean(
bar.params = list(fill = "steelblue"),
errorbar.params = list(colour = "blue")
)

grid.arrange(g1,g2,ncol=2)

```

- If you need more complex behaviour, it might be easier to create a custom geom or stat. You can learn about that in the extending ggplot2 vignette included with the package. Read it by running vignette("extending-ggplot2").

## Plot Functions

- Creating small reusable components is most in line with the ggplot2 spirit: you can recombine them flexibly to create whatever plot you want. But sometimes you’re creating the same plot over and over again, and you don’t need that flexibility. Instead of creating components, you might want to write a
function that takes data and parameters and returns a complete plot.

- For example, you could wrap up the complete code needed to make a piechart:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

piechart <- function(data, mapping) {
ggplot(data, mapping) +
geom_bar(width = 1) +
coord_polar(theta = "y") +
xlab(NULL) +
ylab(NULL)
}
piechart(mpg, aes(factor(1), fill = class))

```

- This is much less flexible than the component based approach, but equally, it’s much more concise. Note that I was careful to return the plot object, rather than printing it. That makes it possible add on other ggplot2 components.

- You can take a similar approach to drawing parallel coordinates plots (PCPs). PCPs require a transformation of the data, so I recommend writing two functions: one that does the transformation and one that generates the plot. Keeping these two pieces separate makes life much easier if you later want to reuse the same transformation for a different visualisation.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

pcp_data <- function(df) {
is_numeric <- vapply(df, is.numeric, logical(1))
# Rescale numeric columns
rescale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
df[is_numeric] <- lapply(df[is_numeric], rescale01)

# Add row identifier
df$.row <- rownames(df)

# Treat numerics as value (aka measure) variables
# gather_ is the standard-evaluation version of gather, and
# is usually easier to program with.
tidyr::gather_(df, "variable", "value", names(df)[is_numeric])
}
pcp <- function(df, ...) {
df <- pcp_data(df)
ggplot(df, aes(variable, value, group = .row)) + geom_line(...)
}
g1<-pcp(mpg)
g2<-pcp(mpg, aes(colour = drv))

grid.arrange(g1,g2,ncol=2)

```

- A complete exploration of this idea is qplot(), which provides a fairly deep wrapper around the most common ggplot() options. I recommend studying the source code if you want to see how far these basic techniques can take you.

## Indirectly Referring to Variables

- The piechart() function above is a little unappealing because it requires the user to know the exact aes() specification that generates a pie chart. It would be more convenient if the user could simply specify the name of the variable to plot. To do that you’ll need to learn a bit more about how aes() works.

- aes() uses non-standard evaluation: rather than looking at the values of its arguments, it looks at their expressions. This makes it difficult to work with programmatically as there’s no way to store the name of a variable in an object and then refer to it later:


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

x_var <- "displ"
aes(x_var)

```

- Instead we need to use aes (), which uses regular evaluation. There are
two basic ways to create a mapping with aes ():

1. Using a quoted call, created by quote(), substitute(), as.name(), or parse().

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

aes_(quote(displ))
#> * x -> displ
aes_(as.name(x_var))
#> * x -> displ
aes_(parse(text = x_var)[[1]])
#> * x -> displ
f <- function(x_var) {
  aes_(substitute(x_var))
}
f(displ)

```

- The difference between as.name() and parse() is subtle. If x var is “a + b”, as.name() will turn it into a variable called ‘a + b‘, parse() will turn it into the function call a + b. (If this is confusing, http://adv-r.had.co.nz/Expressions.html might help).

1. Using a formula, created with ~.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

aes_(~displ)


```

- aes () gives us three options for how a user can supply variables: as a string, as a formula, or as a bare expression. These three options are illustrated below.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

piechart1 <- function(data, var, ...) {
piechart(data, aes_(~factor(1), fill = as.name(var)))
}
piechart1(mpg, "class") + theme(legend.position = "none")
piechart2 <- function(data, var, ...) {
piechart(data, aes_(~factor(1), fill = var))
}
piechart2(mpg, ~class) + theme(legend.position = "none")
piechart3 <- function(data, var, ...) {
piechart(data, aes_(~factor(1), fill = substitute(var)))
}
piechart3(mpg, class) + theme(legend.position = "none")

```

- There’s another advantage to aes () over aes() if you’re writing ggplot2 plots inside a package: using aes (~x, ~y) instead of aes(x, y) avoids the global variables NOTE in R CMD check.


## The Plot Environment

- As you create more sophisticated plotting functions, you’ll need to understand a bit more about ggplot2’s scoping rules. ggplot2 was written well before I understood the full intricacies of non-standard evaluation, so it has a rather simple scoping system. If a variable is not found in the data, it is looked for in the plot environment. There is only one environment for a plot (not one
for each layer), and it is the environment in which ggplot() is called from (i.e. the parent.frame()).

- This means that the following function won’t work because n is not stored in an environment accessible when the expressions in aes() are evaluated.

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

f <- function() {
n <- 10
geom_line(aes(x / n))
}
df <- data.frame(x = 1:3, y = 1:3)
ggplot(df, aes(x, y)) + f()

```

- Note that this is only a problem with the mapping argument. All other arguments are evaluated immediately so their values (not a reference to a name) are stored in the plot object. This means the following function will work:

```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

f <- function() {
colour <- "blue"
geom_line(colour = colour)
}
ggplot(df, aes(x, y)) + f()

```

- If you need to use a different environment for the plot, you can specify it with the environment argument to ggplot(). You’ll need to do this if you’re creating a plot function that takes user provided data. See qplot() for an example.


## Functional Programming

- Since ggplot2 objects are just regular R objects, you can put them in a list. This means you can apply all of R’s great functional programming tools. For example, if you wanted to add different geoms to the same base plot, you could put them in a list and use lapply().


```{r,echo=TRUE,message=FALSE,warning=FALSE,comment=""}
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
library(Lahman)
library(broom)
data(txhousing)

geoms <- list(
geom_point(),
geom_boxplot(aes(group = cut_width(displ, 1))),
list(geom_point(), geom_smooth())
)
p <- ggplot(mpg, aes(displ, hwy))
lapply(geoms, function(g) p + g)

```

- If you’re not familiar with functional programming, read through http://adv-r.had.co.nz/Functional-programming.html and think about how you might apply the techniques to your duplicated plotting code.








